name: Test ClearML on Arm64

on:
  workflow_call:
  push:
    branches:
      - main
      - smoke_tests
    paths:
      - 'content/opensource_packages/clearml-open-source.md'
      - '.github/workflows/test-clearml-open-source.yml'

jobs:
  test-clearml-open-source:
    runs-on: ubuntu-24.04-arm
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set test metadata
        id: metadata
        run: |
          echo "timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
          echo "package_slug=clearml-open-source" >> $GITHUB_OUTPUT
          echo "dashboard_link=/opensource_packages/clearml-open-source" >> $GITHUB_OUTPUT
      
      - name: Install ClearML
        id: install
        run: |
          echo "Installing ClearML..."
          
          # Install using pip (break-system-packages for Ubuntu 24.04)
          if pip install clearml --break-system-packages; then
            echo "ClearML installed successfully"
            echo "install_status=success" >> $GITHUB_OUTPUT
          else
            echo "ClearML installation failed"
            echo "install_status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Get ClearML version
        id: version
        run: |
          VERSION=$(pip show clearml | grep Version | awk '{print $2}' || echo "unknown")
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Detected ClearML version: $VERSION"
      
      - name: Test 1 - Check ClearML Import
        id: test1
        run: |
          START_TIME=$(date +%s)
          STATUS="failed"
          # Ensure we don't exit before setting output
          if python3 -c "import clearml; print('ClearML imported successfully')"; then
            echo "✓ ClearML module import successful"
            STATUS="passed"
          else
            echo "✗ ClearML module import failed"
            STATUS="failed"
          fi
          
          END_TIME=$(date +%s)
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
          if [ "$STATUS" == "failed" ]; then exit 1; fi


      
      - name: Test 2 - Check pip package info
        id: test2
        run: |
          START_TIME=$(date +%s)
          STATUS="failed"
          if pip show clearml; then
            echo "✓ pip package info found"
            STATUS="passed"
          else
            echo "✗ pip package info missing"
            STATUS="failed"
          fi
          
          END_TIME=$(date +%s)
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
          if [ "$STATUS" == "failed" ]; then exit 1; fi


      
      - name: Test 3 - Check clearml-task binary (optional)
        id: test3
        run: |
          START_TIME=$(date +%s)
          STATUS="failed"
          # clearml often installs binaries like clearml-task or clearml-init
          if command -v clearml-task &> /dev/null || command -v clearml-init &> /dev/null; then
            echo "✓ clearml binary found"
            STATUS="passed"
          else
            echo "✓ clearml binary not found (expected for library-only usage)"
            STATUS="passed"
          fi
          
          END_TIME=$(date +%s)
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT



      - name: Test 4 - Architecture Verification
        id: test4
        run: |
          START_TIME=$(date +%s)
          STATUS="failed"
          echo "Checking system architecture..."
          ARCH=$(uname -m)
          if [ "$ARCH" = "aarch64" ]; then
            echo "✓ System architecture is ARM64 ($ARCH)"
            STATUS="passed"
          else
            echo "✗ System architecture is NOT ARM64 ($ARCH)"
            STATUS="failed"
          fi
          
          END_TIME=$(date +%s)
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
          if [ "$STATUS" == "failed" ]; then exit 1; fi


      
      - name: Test 5 - Functional Validation
        id: test5
        run: |
          START_TIME=$(date +%s)
          STATUS="failed"
          # Basic functional check: minimal python script
          if python3 -c "from clearml import Task; print('Task imported')"; then
            echo "✓ Basic functional check passed"
            STATUS="passed"
          else
            echo "✗ Basic functional check failed"
            STATUS="failed"
          fi
          
          END_TIME=$(date +%s)
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
          if [ "$STATUS" == "failed" ]; then exit 1; fi


      - name: Calculate test summary
        if: always()
        id: summary
        run: |
          PASSED=0
          FAILED=0
          TOTAL_DURATION=0
          
          # Test 1
          S1="${{ steps.test1.outputs.status }}"
          D1="${{ steps.test1.outputs.duration }}"
          S1="${S1:-skipped}"
          D1="${D1:-0}"
          if [ "$S1" == "passed" ]; then
            PASSED=$((PASSED + 1))
          elif [ "$S1" != "skipped" ]; then
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + D1))
          
          # Test 2
          S2="${{ steps.test2.outputs.status }}"
          D2="${{ steps.test2.outputs.duration }}"
          S2="${S2:-skipped}"
          D2="${D2:-0}"
          if [ "$S2" == "passed" ]; then
            PASSED=$((PASSED + 1))
          elif [ "$S2" != "skipped" ]; then
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + D2))
          
          # Test 3
          S3="${{ steps.test3.outputs.status }}"
          D3="${{ steps.test3.outputs.duration }}"
          S3="${S3:-skipped}"
          D3="${D3:-0}"
          if [ "$S3" == "passed" ]; then
            PASSED=$((PASSED + 1))
          elif [ "$S3" != "skipped" ]; then
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + D3))
          
          # Test 4
          S4="${{ steps.test4.outputs.status }}"
          D4="${{ steps.test4.outputs.duration }}"
          S4="${S4:-skipped}"
          D4="${D4:-0}"
          if [ "$S4" == "passed" ]; then
            PASSED=$((PASSED + 1))
          elif [ "$S4" != "skipped" ]; then
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + D4))
          
          # Test 5
          S5="${{ steps.test5.outputs.status }}"
          D5="${{ steps.test5.outputs.duration }}"
          S5="${S5:-skipped}"
          D5="${D5:-0}"
          if [ "$S5" == "passed" ]; then
            PASSED=$((PASSED + 1))
          elif [ "$S5" != "skipped" ]; then
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + D5))
          
          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "duration=$TOTAL_DURATION" >> $GITHUB_OUTPUT
          
          if [ $FAILED -eq 0 ]; then
            echo "overall_status=success" >> $GITHUB_OUTPUT
            echo "badge_status=passing" >> $GITHUB_OUTPUT
          else
            echo "overall_status=failure" >> $GITHUB_OUTPUT
            echo "badge_status=failing" >> $GITHUB_OUTPUT
            exit 1
          fi
      - name: Generate test results JSON
        if: always()
        run: |
          mkdir -p test-results
          
          cat > test-results/clearml-open-source.json << EOF
          {
            "schema_version": "1.0",
            "package": {
              "name": "ClearML",
              "version": "${{ steps.version.outputs.version }}",
              "language": "AI/ML",
              "category": "AI/ML"
            },
            "run": {
              "id": "${{ github.run_id }}",
              "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}?job=${{ github.job }}",
              "timestamp": "${{ steps.metadata.outputs.timestamp }}",
              "status": "${{ steps.summary.outputs.overall_status }}",
              "runner": {
                "os": "ubuntu-24.04",
                "arch": "arm64"
              }
            },
            "tests": {
              "passed": ${{ steps.summary.outputs.passed }},
              "failed": ${{ steps.summary.outputs.failed }},
              "skipped": 0,
              "duration_seconds": ${{ steps.summary.outputs.duration || 0 }},
              "details": [
                {
                  "name": "Check ClearML Import",
                  "status": "${{ steps.test1.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test1.outputs.duration || 0 }}
                },
                {
                  "name": "Check pip package info",
                  "status": "${{ steps.test2.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test2.outputs.duration || 0 }}
                },
                {
                  "name": "Check clearml-task binary (optional)",
                  "status": "${{ steps.test3.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test3.outputs.duration || 0 }}
                },
                {
                  "name": "Architecture Verification",
                  "status": "${{ steps.test4.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test4.outputs.duration || 0 }}
                },
                {
                  "name": "Functional Validation",
                  "status": "${{ steps.test5.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test5.outputs.duration || 0 }}
                }
              ]
            },
            "metadata": {
              "dashboard_link": "${{ steps.metadata.outputs.dashboard_link }}",
              "badge_status": "${{ steps.summary.outputs.badge_status }}"
            }
          }
          EOF
          
          echo "Generated test results:"
          cat test-results/clearml-open-source.json
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: clearml-open-source-test-results
          path: test-results/clearml-open-source.json
          retention-days: 90
      
      - name: Create test summary
        if: always()
        run: |
          echo "## ClearML Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Version:** ${{ steps.version.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ steps.summary.outputs.overall_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Passed:** ${{ steps.summary.outputs.passed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Failed:** ${{ steps.summary.outputs.failed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration:** ${{ steps.summary.outputs.duration || 0 }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Runner:** ubuntu-24.04 (arm64)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Details" >> $GITHUB_STEP_SUMMARY
          echo "1. Check ClearML Import: ${{ steps.test1.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "2. Check pip package info: ${{ steps.test2.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "3. Check clearml-task binary (optional): ${{ steps.test3.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          
          echo "4. Test 4 - Architecture Verification: ${{ steps.test4.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "5. Test 5 - Functional Validation: ${{ steps.test5.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY

