name: Test ONNX on Arm64
on:
  workflow_call:
  push:
    branches:
      - main
      - smoke_tests
    paths:
      - 'content/opensource_packages/onnx.md'
      - '.github/workflows/test-onnx.yml'
jobs:
  test-onnx:
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set test metadata
        id: metadata
        run: |
          echo "timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
          echo "package_slug=onnx" >> $GITHUB_OUTPUT
          echo "dashboard_link=/opensource_packages/onnx" >> $GITHUB_OUTPUT

      # ============================================================
      # CUSTOMIZE THIS: Install your package
      # ============================================================
      - name: Install ONNX
        id: install
        run: |
          echo "Installing ONNX..."
          
          # Install prerequisites
          sudo apt-get update
          sudo apt-get install -y python3-pip python3-venv python3-dev
          
          # Create venv
          python3 -m venv venv
          source venv/bin/activate
          
          # Install ONNX and ONNX Runtime
          if pip install onnx onnxruntime; then
             echo "ONNX installed successfully"
             echo "install_status=success" >> $GITHUB_OUTPUT
          else
             echo "ONNX installation failed"
             echo "install_status=failed" >> $GITHUB_OUTPUT
             exit 1
          fi

      # ============================================================
      # CUSTOMIZE THIS: Get the package version
      # ============================================================
      - name: Get ONNX version
        id: version
        run: |
          source venv/bin/activate
          VERSION=$(python3 -c "import onnx; print(onnx.__version__)" 2>&1 || echo "unknown")
          VERSION=$(echo "$VERSION" | sed 's/[",]//g' | tr -d '\\' | sed 's/\x1b\[[0-9;]*m//g')
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Detected ONNX version: $VERSION"

      # ============================================================
      # ADD YOUR TESTS BELOW
      # Each test should:
      # 1. Have a unique id (test1, test2, etc.)
      # 2. Track start/end time for duration
      # 3. Set status=passed or status=failed
      # 4. Exit 1 on failure
      # ============================================================

      - name: Test 1 - Import ONNX
        id: test1
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          source venv/bin/activate
          
          if python3 -c "import onnx; print('ONNX imported')" | grep -q "ONNX imported"; then
            echo "✓ ONNX import successful"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ ONNX import failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 2 - Create Simple Model
        id: test2
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          source venv/bin/activate
          
          cat <<EOF > create_model.py
          import onnx
          from onnx import helper
          from onnx import TensorProto
          import os
          
          # Create a simple model (Add operation)
          # Use opset 13 for better compatibility with onnxruntime
          node = helper.make_node(
              'Add',
              inputs=['x', 'y'],
              outputs=['z'],
          )
          
          x = helper.make_tensor_value_info('x', TensorProto.FLOAT, [1])
          y = helper.make_tensor_value_info('y', TensorProto.FLOAT, [1])
          z = helper.make_tensor_value_info('z', TensorProto.FLOAT, [1])
          
          graph = helper.make_graph(
              [node],
              'test-model',
              [x, y],
              [z],
          )
          
          # Explicitly set opset 13
          model = helper.make_model(graph, producer_name='onnx-test', opset_imports=[helper.make_opsetid("", 13)])
          model_path = os.path.abspath('simple.onnx')
          onnx.save(model, model_path)
          print(f"Model saved to {model_path}")
          EOF
          
          if python3 create_model.py | grep -q "Model saved"; then
             echo "✓ Model creation successful"
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ Model creation failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 3 - Check Model
        id: test3
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          source venv/bin/activate
          
          cat <<EOF > check_model.py
          import onnx
          import os
          model_path = os.path.abspath("simple.onnx")
          if not os.path.exists(model_path):
              print(f"Error: {model_path} not found")
              exit(1)
          model = onnx.load(model_path)
          onnx.checker.check_model(model)
          print("Model checked")
          EOF
          
          if python3 check_model.py | grep -q "Model checked"; then
             echo "✓ Model check successful"
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ Model check failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 4 - Run Inference (ONNX Runtime)
        id: test4
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          source venv/bin/activate
          
          cat <<EOF > run_model.py
          import onnxruntime as ort
          import numpy as np
          import os
          
          model_path = os.path.abspath("simple.onnx")
          if not os.path.exists(model_path):
              print(f"Error: {model_path} not found")
              exit(1)
              
          sess = ort.InferenceSession(model_path)
          x = np.array([2.0], dtype=np.float32)
          y = np.array([3.0], dtype=np.float32)
          
          res = sess.run(None, {'x': x, 'y': y})
          print(f"Result: {res[0][0]}")
          EOF
          
          if python3 run_model.py | grep -q "Result: 5.0"; then
             echo "✓ Inference successful"
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ Inference failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      # ============================================================
      # UPDATE THIS: Calculate summary based on your number of tests
      # Add/remove test result checks to match your tests above
      - name: Calculate test summary
        if: always()
        id: summary
        run: |
          PASSED=0
          FAILED=0
          TOTAL_DURATION=0

          # Import ONNX
          if [ "${{ steps.test1.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test1.outputs.duration || 0 }}))

          # Create Simple Model
          if [ "${{ steps.test2.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test2.outputs.duration || 0 }}))

          # Check Model
          if [ "${{ steps.test3.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test3.outputs.duration || 0 }}))

          # Run Inference (ONNX Runtime)
          if [ "${{ steps.test4.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test4.outputs.duration || 0 }}))

          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "duration=$TOTAL_DURATION" >> $GITHUB_OUTPUT

          if [ $FAILED -eq 0 ]; then
            echo "overall_status=success" >> $GITHUB_OUTPUT
            echo "badge_status=passing" >> $GITHUB_OUTPUT
          else
            echo "overall_status=failure" >> $GITHUB_OUTPUT
            echo "badge_status=failing" >> $GITHUB_OUTPUT
            exit 1
          fi
      - name: Generate test results JSON
        if: always()
        run: |
          # Fetch the direct job URL for deep-linking
          JOB_ID="${{ github.job }}"
          # Using GH_TOKEN to find the exact job URL. 
          # Reusable workflows are often named "JobID / JobID" or just "JobID"
          JOB_URL=$(GH_TOKEN=${{ github.token }} gh api repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/jobs --jq ".jobs[] | select(.name == \"$JOB_ID / $JOB_ID\" or .name == \"$JOB_ID\") | .html_url" | head -n 1)
          
          # Fallback if URL calculation fails
          if [ -z "$JOB_URL" ]; then
            JOB_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}?check_suite_focus=true&query=job:$JOB_ID"
          fi

          mkdir -p test-results
          
          cat > test-results/onnx.json << EOF
          {
            "schema_version": "1.0",
            "package": {
              "name": "ONNX",
              "version": "${{ steps.version.outputs.version }}",
              "language": "REPLACE_ME",
              "category": "REPLACE_ME"
            },
            "run": {
              "id": "${{ github.run_id }}",
              "url": "$JOB_URL",
              "timestamp": "${{ steps.metadata.outputs.timestamp }}",
              "status": "${{ steps.summary.outputs.overall_status }}",
              "runner": {
                "os": "ubuntu-24.04",
                "arch": "arm64"
              }
            },
            "tests": {
              "passed": ${{ steps.summary.outputs.passed }},
              "failed": ${{ steps.summary.outputs.failed }},
              "skipped": 0,
              "duration_seconds": ${{ steps.summary.outputs.duration || 0 }},
              "details": [
                {
                  "name": "Import ONNX",
                  "status": "${{ steps.test1.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test1.outputs.duration || 0 }}
                },
                {
                  "name": "Create Simple Model",
                  "status": "${{ steps.test2.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test2.outputs.duration || 0 }}
                },
                {
                  "name": "Check Model",
                  "status": "${{ steps.test3.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test3.outputs.duration || 0 }}
                },
                {
                  "name": "Run Inference (ONNX Runtime)",
                  "status": "${{ steps.test4.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test4.outputs.duration || 0 }}
                }
              ]
            },
            "metadata": {
              "dashboard_link": "${{ steps.metadata.outputs.dashboard_link }}",
              "badge_status": "${{ steps.summary.outputs.badge_status }}"
            }
          }
          EOF
          
          echo "Generated test results:"
          cat test-results/onnx.json
      # ============================================================
      # STANDARD STEPS - Usually don't need to modify below here
      # ============================================================
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: onnx-test-results
          path: test-results/onnx.json
          retention-days: 90
      
      - name: Create test summary
        if: always()
        run: |
          echo "## ONNX Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Version:** ${{ steps.version.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ steps.summary.outputs.overall_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Passed:** ${{ steps.summary.outputs.passed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Failed:** ${{ steps.summary.outputs.failed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration:** ${{ steps.summary.outputs.duration || 0 }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Runner:** ubuntu-24.04 (arm64)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Details" >> $GITHUB_STEP_SUMMARY
          echo "1. Test 1 - Import ONNX: ${{ steps.test1.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "2. Test 2 - Create Simple Model: ${{ steps.test2.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "3. Test 3 - Check Model: ${{ steps.test3.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "4. Test 4 - Run Inference (ONNX Runtime): ${{ steps.test4.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY



