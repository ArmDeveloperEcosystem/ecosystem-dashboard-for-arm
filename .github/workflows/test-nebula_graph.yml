name: Test Nebula Graph Console on Arm64
on:
  workflow_call:
  push:
    branches:
      - main
      - smoke_tests
    paths:
      - 'content/opensource_packages/nebula_graph.md'
      - '.github/workflows/test-nebula_graph.yml'
jobs:
  test-nebula_graph:
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set test metadata
        id: metadata
        run: |
          echo "timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
          echo "package_slug=nebula_graph" >> $GITHUB_OUTPUT
          echo "dashboard_link=/opensource_packages/nebula_graph" >> $GITHUB_OUTPUT

      # ============================================================
      # CUSTOMIZE THIS: Install your package
      # ============================================================
      - name: Install Nebula Console
        id: install
        run: |
          echo "Installing Nebula Console..."
          
          # Install prerequisites
          sudo apt-get update
          sudo apt-get install -y wget
          
          # Download Nebula Console
          VERSION="v3.8.0"
          URL="https://github.com/vesoft-inc/nebula-console/releases/download/${VERSION}/nebula-console-linux-arm64-v3.8.0"
          
          if wget -q -O nebula-console $URL; then
             chmod +x nebula-console
             sudo mv nebula-console /usr/local/bin/
             echo "Nebula Console installed successfully"
             echo "install_status=success" >> $GITHUB_OUTPUT
          else
             echo "Nebula Console download failed"
             echo "install_status=failed" >> $GITHUB_OUTPUT
             exit 1
          fi

      # ============================================================
      # CUSTOMIZE THIS: Get the package version
      # ============================================================
      - name: Get Nebula Console version
        id: version
        run: |
          # nebula-console doesn't seem to have a --version flag in some versions, or it prints to stderr
          # Let's try -version or --version
          if command -v nebula-console &> /dev/null; then
             # It might just print help if flag is unknown, or version if known
             # v3.8.0 usually supports -version
             VERSION=$(nebula-console -version 2>&1 | grep "nebula-console" | awk '{print $3}' || echo "unknown")
             if [ "$VERSION" = "unknown" ]; then
                VERSION="v3.8.0" # Fallback if we can't parse it but binary works
            fi
          else
             VERSION="unknown"
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Detected Nebula Console version: $VERSION"

      # ============================================================
      # ADD YOUR TESTS BELOW
      # Each test should:
      # 1. Have a unique id (test1, test2, etc.)
      # 2. Track start/end time for duration
      # 3. Set status=passed or status=failed
      # 4. Exit 1 on failure
      # ============================================================

      - name: Test 1 - Check nebula-console binary
        id: test1
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          if command -v nebula-console &> /dev/null; then
            echo "✓ nebula-console binary found"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ nebula-console binary not found"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 2 - Check help command
        id: test2
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          if nebula-console -h 2>&1 | grep -q "Usage"; then
             echo "✓ help command works"
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ help command failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 3 - Check interactive mode (dry run)
        id: test3
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          # We can't connect to a server, but we can try to run it and expect a connection error
          # This proves the binary is executable and libraries are linked
          
          if nebula-console -addr 127.0.0.1 -port 9669 -u user -p password 2>&1 | grep -q "connection refused"; then
             echo "✓ Binary ran and attempted connection"
             echo "status=passed" >> $GITHUB_OUTPUT
          elif nebula-console -addr 127.0.0.1 -port 9669 -u user -p password 2>&1 | grep -q "dial tcp"; then
             echo "✓ Binary ran and attempted connection"
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             # It might fail with other errors
             OUTPUT=$(nebula-console -addr 127.0.0.1 -port 9669 -u user -p password 2>&1 || true)
             echo "Output: $OUTPUT"
             if [[ "$OUTPUT" == *"[ERROR]"* ]]; then
                echo "✓ Binary ran and reported error (expected)"
                echo "status=passed" >> $GITHUB_OUTPUT
            else
                echo "✗ Binary failed to run or crashed"
                echo "status=failed" >> $GITHUB_OUTPUT
                exit 1
            fi
          fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

            # ============================================================
      # UPDATE THIS: Calculate summary based on your number of tests
      # Add/remove test result checks to match your tests above
      # ============================================================
      
      - name: Test 4 - Architecture Verification
        id: test4
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          echo "Checking system architecture..."
          ARCH=$(uname -m)
          if [ "$ARCH" = "aarch64" ]; then
            echo "✓ System architecture is ARM64 ($ARCH)"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ System architecture is NOT ARM64 ($ARCH)"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 5 - Functional Validation
        id: test5
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          # Basic functional check: try to run with --version or --help and check exit code
          if nebula_graph --version > /dev/null 2>&1 || nebula_graph --help > /dev/null 2>&1; then
            echo "✓ Basic functional check passed"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ Basic functional check failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

          END_TIME=$(date +%s)
      - name: Calculate test summary
        if: always()
        id: summary
        run: |
          PASSED=0
          FAILED=0
          TOTAL_DURATION=0

          # Check nebula-console binary
          if [ "${{ steps.test1.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test1.outputs.duration || 0 }}))

          # Check help command
          if [ "${{ steps.test2.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test2.outputs.duration || 0 }}))

          # Check interactive mode (dry run)
          if [ "${{ steps.test3.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test3.outputs.duration || 0 }}))

          # Architecture Verification
          if [ "${{ steps.test4.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test4.outputs.duration || 0 }}))

          # Functional Validation
          if [ "${{ steps.test5.outputs.status }}" == "passed" ]; then
            PASSED=$((PASSED + 1))
          else
            FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${{ steps.test5.outputs.duration || 0 }}))

          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "duration=$TOTAL_DURATION" >> $GITHUB_OUTPUT

          if [ $FAILED -eq 0 ]; then
            echo "overall_status=success" >> $GITHUB_OUTPUT
            echo "badge_status=passing" >> $GITHUB_OUTPUT
          else
            echo "overall_status=failure" >> $GITHUB_OUTPUT
            echo "badge_status=failing" >> $GITHUB_OUTPUT
            exit 1
          fi
      - name: Generate test results JSON
        if: always()
        run: |
          mkdir -p test-results
          
          cat > test-results/nebula_graph.json << EOF
          {
            "schema_version": "1.0",
            "package": {
              "name": "Nebula Graph Console",
              "version": "${{ steps.version.outputs.version }}",
              "language": "REPLACE_ME",
              "category": "REPLACE_ME"
            },
            "run": {
              "id": "${{ github.run_id }}",
              "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "timestamp": "${{ steps.metadata.outputs.timestamp }}",
              "status": "${{ steps.summary.outputs.overall_status }}",
              "runner": {
                "os": "ubuntu-24.04",
                "arch": "arm64"
              }
            },
            "tests": {
              "passed": ${{ steps.summary.outputs.passed }},
              "failed": ${{ steps.summary.outputs.failed }},
              "skipped": 0,
              "duration_seconds": ${{ steps.summary.outputs.duration || 0 }},
              "details": [
                {
                  "name": "Check nebula-console binary",
                  "status": "${{ steps.test1.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test1.outputs.duration || 0 }}
                },
                {
                  "name": "Check help command",
                  "status": "${{ steps.test2.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test2.outputs.duration || 0 }}
                },
                {
                  "name": "Check interactive mode (dry run)",
                  "status": "${{ steps.test3.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test3.outputs.duration || 0 }}
                },
                {
                  "name": "Architecture Verification",
                  "status": "${{ steps.test4.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test4.outputs.duration || 0 }}
                },
                {
                  "name": "Functional Validation",
                  "status": "${{ steps.test5.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test5.outputs.duration || 0 }}
                }
              ]
            },
            "metadata": {
              "dashboard_link": "${{ steps.metadata.outputs.dashboard_link }}",
              "badge_status": "${{ steps.summary.outputs.badge_status }}"
            }
          }
          EOF
          
          echo "Generated test results:"
          cat test-results/nebula_graph.json
      # ============================================================
      # STANDARD STEPS - Usually don't need to modify below here
      # ============================================================
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nebula_graph-test-results
          path: test-results/nebula_graph.json
          retention-days: 90
      
      - name: Commit test results to repository
        if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/smoke_tests')
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          mkdir -p data/test-results
          cp test-results/nebula_graph.json data/test-results/nebula_graph.json
          
          git add data/test-results/nebula_graph.json
          
          if ! git diff --staged --quiet; then
            git commit -m "Update nebula_graph test results [skip ci]"
            
            # Pull with rebase and push, retry up to 20 times
            PUSH_SUCCESS=false
            for i in {1..20}; do
              if git pull --rebase origin ${{ github.ref_name }}; then
                # Rebase succeeded, try to push
                if git push; then
                  echo "Successfully pushed test results"
                  PUSH_SUCCESS=true
                  break
                fi
              else
                # Rebase failed, likely due to conflict
                echo "Rebase failed, resolving conflicts..."
                
                # Accept our version of the file (the new test results)
                git checkout --ours data/test-results/nebula_graph.json
                git add data/test-results/nebula_graph.json
                
                # Continue the rebase
                git rebase --continue || true
              fi
              
              # Wait before retry
              echo "Retry attempt $i of 20..."
              sleep $(( (i % 5 + 1) * 2 + (RANDOM % 3) ))
            done
            
            if [ "$PUSH_SUCCESS" = false ]; then
              echo "❌ Error: Failed to push test results after 20 attempts."
              exit 1
            fi
          else
            echo "No changes to commit"
          fi
      
      - name: Create test summary
        if: always()
        run: |
          echo "## Nebula Graph Console Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Version:** ${{ steps.version.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ steps.summary.outputs.overall_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Passed:** ${{ steps.summary.outputs.passed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Failed:** ${{ steps.summary.outputs.failed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration:** ${{ steps.summary.outputs.duration || 0 }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Runner:** ubuntu-24.04 (arm64)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Details" >> $GITHUB_STEP_SUMMARY
          echo "1. Test 1 - Check nebula-console binary: ${{ steps.test1.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "2. Test 2 - Check help command: ${{ steps.test2.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "3. Test 3 - Check interactive mode (dry run): ${{ steps.test3.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY

          echo "4. Test 4 - Architecture Verification: ${{ steps.test4.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "5. Test 5 - Functional Validation: ${{ steps.test5.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY




