# Package Test Template
#
# This is a TEMPLATE file stored in the tests/ directory - it will not run as a workflow.
# To use it:
# 1. Copy this file to .github/workflows/test-<your-package>.yml
# 2. Replace all Featuretools placeholders with your package name (e.g., "Redis")
# 3. Replace all featuretools placeholders with your package slug (lowercase, e.g., "redis")
# 4. Update the install commands for your package
# 5. Update the version detection command
# 6. Add/modify/remove test steps as needed
# 7. Update package metadata in the JSON generation step
# 8. Uncomment the appropriate trigger(s) in the 'on:' section
#
# See .github/workflows/test-nginx.yml and test-envoy.yml for real examples. Template
#
# This is a TEMPLATE file - it will not run automatically.
# To use it:
# 1. Copy this file to test-<your-package>.yml
# 2. Replace all Featuretools placeholders with your package name (e.g., "Redis")
# 3. Replace all featuretools placeholders with your package slug (lowercase, e.g., "redis")
# 4. Update the install commands for your package
# 5. Update the version detection command
# 6. Add/modify/remove test steps as needed
# 7. Update package metadata in the JSON generation step
# 8. Uncomment the 'push:' trigger section below (remove the workflow_dispatch if desired)
#
# See test-nginx.yml and test-envoy.yml for real examples.

name: Test Featuretools on Arm64

# This is a TEMPLATE - it has no triggers and will not run.
# When you copy this file, uncomment the appropriate triggers below:
on:
  # workflow_dispatch:  # Uncomment for manual testing
  workflow_call:
  push:
    branches:
      - main
      - smoke_tests
    paths:
      - 'content/opensource_packages/featuretools.md'
      - '.github/workflows/test-featuretools.yml'

jobs:
  test-featuretools:
    runs-on: ubuntu-24.04-arm
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set test metadata
        id: metadata
        run: |
          echo "timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
          echo "package_slug=featuretools" >> $GITHUB_OUTPUT
          echo "dashboard_link=/opensource_packages/featuretools" >> $GITHUB_OUTPUT
      
      # ============================================================
      # CUSTOMIZE THIS: Install your package
      # ============================================================
      - name: Install Featuretools
        id: install
        run: |
          echo "Installing Featuretools..."
          
          # Install using pip
          if pip install featuretools; then
             echo "Featuretools installed successfully"
             echo "install_status=success" >> $GITHUB_OUTPUT
          else
             echo "Featuretools installation failed"
             echo "install_status=failed" >> $GITHUB_OUTPUT
             exit 1
          fi
      
      # ============================================================
      # CUSTOMIZE THIS: Get the package version
      # ============================================================
      - name: Get Featuretools version
        id: version
        run: |
          # Get version from pip show
          VERSION=$(pip show featuretools | grep Version | awk '{print $2}' || echo "unknown")
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Detected Featuretools version: $VERSION"
      
      # ============================================================
      # ADD YOUR TESTS BELOW
      # Each test should:
      # 1. Have a unique id (test1, test2, etc.)
      # 2. Track start/end time for duration
      # 3. Set status=passed or status=failed
      # 4. Exit 1 on failure
      # ============================================================
      
      - name: Test 1 - Check pip installation
        id: test1
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          if pip show featuretools > /dev/null; then
            echo "✓ featuretools found in pip"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ featuretools not found in pip"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
      
      - name: Test 2 - Import featuretools in Python
        id: test2
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          if python3 -c "import featuretools; print('featuretools imported successfully')"; then
            echo "✓ featuretools import works"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ featuretools import failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
      
      - name: Test 3 - Check featuretools version attribute
        id: test3
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          if python3 -c "import featuretools; print(featuretools.__version__)"; then
            echo "✓ featuretools.__version__ works"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ featuretools.__version__ check failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
      
      - name: Test 4 - Load mock data
        id: test4
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          # Create a python script to load mock data
          cat > test_ft_mock.py << EOF
          import featuretools as ft
          try:
              data = ft.demo.load_mock_customer(return_entityset=True)
              print("Mock data loaded successfully")
          except Exception as e:
              print(f"Error loading mock data: {e}")
              exit(1)
          EOF
          
          if python3 test_ft_mock.py; then
             echo "✓ featuretools mock data load passed"
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ featuretools mock data load failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
      
      - name: Test 5 - Run Deep Feature Synthesis (DFS)
        id: test5
        continue-on-error: true
        run: |
          START_TIME=$(date +%s)
          
          # Create a python script to run simple DFS
          cat > test_ft_dfs.py << EOF
          import featuretools as ft
          import pandas as pd
          
          # Create simple data
          df = pd.DataFrame({'id': [1, 2, 3], 'val': [10, 20, 30]})
          es = ft.EntitySet(id="test")
          es = es.add_dataframe(dataframe_name="data", dataframe=df, index="id")
          
          try:
              feature_matrix, features_defs = ft.dfs(entityset=es, target_dataframe_name="data")
              print("DFS ran successfully")
          except Exception as e:
              print(f"Error running DFS: {e}")
              exit(1)
          EOF
          
          if python3 test_ft_dfs.py; then
             echo "✓ featuretools DFS passed"
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ featuretools DFS failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
            fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT
      
      # Add more tests as needed (test4, test5, etc.)
      # Examples:
      # - Run a simple command
      # - Check configuration files
      # - Start/stop a service
      # - Test basic functionality
      
      # ============================================================
      # UPDATE THIS: Calculate summary based on your number of tests
      # Add/remove test result checks to match your tests above
      # ============================================================
      - name: Calculate test summary
        if: always()
        id: summary
        env:
          TEST1_STATUS: ${TEST1_STATUS}
          TEST1_DURATION: ${TEST1_DURATION}
          TEST2_STATUS: ${TEST2_STATUS}
          TEST2_DURATION: ${TEST2_DURATION}
          TEST3_STATUS: ${TEST3_STATUS}
          TEST3_DURATION: ${TEST3_DURATION}
        run: |
          PASSED=0
          FAILED=0
          TOTAL_DURATION=0
                    # Test 1
          if [ "${TEST1_STATUS}" == "passed" ]; then
          PASSED=$((PASSED + 1))
          else
          FAILED=$((FAILED + 1))
                    fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${TEST1_DURATION}))
                    # Test 2
          if [ "${TEST2_STATUS}" == "passed" ]; then
          PASSED=$((PASSED + 1))
          else
          FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${TEST2_DURATION}))
                    # Test 3
          if [ "${TEST3_STATUS}" == "passed" ]; then
          PASSED=$((PASSED + 1))
          else
          FAILED=$((FAILED + 1))
          fi
          TOTAL_DURATION=$((TOTAL_DURATION + ${TEST3_DURATION}))
                    # Add more tests here if you added test4, test5, etc. above
                    echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "duration=$TOTAL_DURATION" >> $GITHUB_OUTPUT
                    if [ $FAILED -eq 0 ]; then
          echo "overall_status=success" >> $GITHUB_OUTPUT
          echo "badge_status=passing" >> $GITHUB_OUTPUT
          else
          echo "overall_status=failure" >> $GITHUB_OUTPUT
          echo "badge_status=failing" >> $GITHUB_OUTPUT
          exit 1
          fi
                    # ============================================================
          # UPDATE THIS: Generate JSON with your package metadata and test details
          # ============================================================
      - name: Generate test results JSON
        if: always()
        run: |
          mkdir -p test-results
          
          cat > test-results/featuretools.json << EOF
          {
            "schema_version": "1.0",
            "package": {
              "name": "Featuretools",
              "version": "${{ steps.version.outputs.version }}",
              "language": "AI/ML",
              "category": "AI/ML"
            },
            "run": {
              "id": "${{ github.run_id }}",
              "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "timestamp": "${{ steps.metadata.outputs.timestamp }}",
              "status": "${{ steps.summary.outputs.overall_status }}",
              "runner": {
                "os": "ubuntu-24.04",
                "arch": "arm64"
              }
            },
            "tests": {
              "passed": ${{ steps.summary.outputs.passed }},
              "failed": ${{ steps.summary.outputs.failed }},
              "skipped": 0,
              "duration_seconds": ${{ steps.summary.outputs.duration || 0 }},
              "details": [
                {
                  "name": "Check featuretools binary exists",
                  "status": "${{ steps.test1.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test1.outputs.duration || 0 }}
                },
                {
                  "name": "Check featuretools version command",
                  "status": "${{ steps.test2.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test2.outputs.duration || 0 }}
                },
                {
                  "name": "Check featuretools help output",
                  "status": "${{ steps.test3.outputs.status || 'skipped' }}",
                  "duration_seconds": ${{ steps.test3.outputs.duration || 0 }}
                }
              ]
            },
            "metadata": {
              "dashboard_link": "${{ steps.metadata.outputs.dashboard_link }}",
              "badge_status": "${{ steps.summary.outputs.badge_status }}"
            }
          }
          EOF
          
          echo "Generated test results:"
          cat test-results/featuretools.json
      
      # ============================================================
      # STANDARD STEPS - Usually don't need to modify below here
      # ============================================================
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: featuretools-test-results
          path: test-results/featuretools.json
          retention-days: 90
      
      - name: Commit test results to repository
        if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/smoke_tests')
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          mkdir -p data/test-results
          cp test-results/featuretools.json data/test-results/featuretools.json
          
          git add data/test-results/featuretools.json
          
          if ! git diff --staged --quiet; then
            git commit -m "Update featuretools test results [skip ci]"
            
            # Pull with rebase and push, retry up to 5 times
            for i in {1..5}; do
              if git pull --rebase origin ${{ github.ref_name }}; then
                # Rebase succeeded, try to push
                if git push; then
                  echo "Successfully pushed test results"
                  break
                fi
              else
                # Rebase failed, likely due to conflict
                echo "Rebase failed, resolving conflicts..."
                
                # Accept our version of the file (the new test results)
                git checkout --ours data/test-results/featuretools.json
                git add data/test-results/featuretools.json
                
                # Continue the rebase
                git rebase --continue || true
              fi
              
              # Wait before retry
              echo "Retry attempt $i of 5..."
              sleep $((i * 2 + (RANDOM % 5)))
            done
          else
            echo "No changes to commit"
          fi
      
      - name: Create test summary
        if: always()
        run: |
          echo "## Featuretools Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Version:** ${{ steps.version.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ steps.summary.outputs.overall_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Passed:** ${{ steps.summary.outputs.passed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests Failed:** ${{ steps.summary.outputs.failed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration:** ${{ steps.summary.outputs.duration || 0 }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Runner:** ubuntu-24.04 (arm64)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Details" >> $GITHUB_STEP_SUMMARY
          echo "1. Check featuretools binary exists: ${{ steps.test1.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "2. Check featuretools version command: ${{ steps.test2.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "3. Check featuretools help output: ${{ steps.test3.outputs.status || 'skipped' }}" >> $GITHUB_STEP_SUMMARY



