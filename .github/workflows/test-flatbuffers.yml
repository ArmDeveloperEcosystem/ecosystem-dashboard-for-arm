name: Test FlatBuffers on Arm64
on:
  push:
    branches:
      - main
      - smoke_tests
    paths:
      - 'content/opensource_packages/flatBuffers.md'
      - '.github/workflows/test-flatBuffers.yml'
jobs:
  test-flatbuffers:
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set test metadata
        id: metadata
        run: |
          echo "timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
          echo "package_slug=flatBuffers" >> $GITHUB_OUTPUT
          echo "dashboard_link=/opensource_packages/flatBuffers" >> $GITHUB_OUTPUT

      # ============================================================
      # CUSTOMIZE THIS: Install your package
      # ============================================================
      - name: Install FlatBuffers
        id: install
        run: |
          echo "Installing FlatBuffers..."
          
          # Install using apt-get
          sudo apt-get update
          if sudo apt-get install -y flatbuffers-compiler; then
             echo "FlatBuffers installed successfully"
             echo "install_status=success" >> $GITHUB_OUTPUT
          else
             echo "FlatBuffers installation failed"
             echo "install_status=failed" >> $GITHUB_OUTPUT
             exit 1
          fi

      # ============================================================
      # CUSTOMIZE THIS: Get the package version
      # ============================================================
      - name: Get FlatBuffers version
        id: version
        run: |
          # Get version from flatc command
          VERSION=$(flatc --version 2>&1 | grep -oP 'flatc version \K[0-9.]+' || echo "unknown")
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Detected FlatBuffers version: $VERSION"

      # ============================================================
      # ADD YOUR TESTS BELOW
      # Each test should:
      # 1. Have a unique id (test1, test2, etc.)
      # 2. Track start/end time for duration
      # 3. Set status=passed or status=failed
      # 4. Exit 1 on failure
      # ============================================================

      - name: Test 1 - Check flatc binary exists
        id: test1
        run: |
          START_TIME=$(date +%s)
          
          if command -v flatc &> /dev/null; then
            echo "✓ flatc binary found"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ flatc binary not found"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 2 - Check flatc version command
        id: test2
        run: |
          START_TIME=$(date +%s)
          
          if flatc --version 2>&1 | grep -q "flatc version"; then
            echo "✓ flatc version command works"
            flatc --version
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ flatc version command failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 3 - Check flatc help output
        id: test3
        run: |
          START_TIME=$(date +%s)
          
          if flatc --help 2>&1 | grep -qi "usage"; then
            echo "✓ flatc help command works"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "✗ flatc help command failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 4 - Compile sample schema
        id: test4
        run: |
          START_TIME=$(date +%s)
          
          # Create a sample schema
          cat > monster.fbs << EOF
          namespace MyGame.Sample;
          enum Color:byte { Red = 0, Green, Blue = 2 }
          union Equipment { Weapon }
          struct Vec3 {
            x:float;
            y:float;
            z:float;
          }
          table Monster {
            pos:Vec3;
            mana:short = 150;
            hp:short = 100;
            name:string;
            friendly:bool = false (deprecated);
            inventory:[ubyte];
            color:Color = Blue;
            weapons:[Weapon];
            equipped:Equipment;
            path:[Vec3];
          }
          table Weapon {
            name:string;
            damage:short;
          }
          root_type Monster;
          EOF
          
          # Compile it to C++
          if flatc --cpp monster.fbs; then
             echo "✓ Schema compilation passed"
             ls -l monster_generated.h
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ Schema compilation failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
          fi
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Test 5 - Compile schema to Python
        id: test5
        run: |
          START_TIME=$(date +%s)
          
          # Compile the same schema to Python
          if flatc --python monster.fbs; then
             echo "✓ Schema compilation to Python passed"
             ls -R MyGame
             echo "status=passed" >> $GITHUB_OUTPUT
          else
             echo "✗ Schema compilation to Python failed"
             echo "status=failed" >> $GITHUB_OUTPUT
             exit 1
          fi
          
          # Cleanup
          rm monster.fbs monster_generated.h
          rm -rf MyGame
          
          END_TIME=$(date +%s)
          echo "duration=$((END_TIME - START_TIME))" >> $GITHUB_OUTPUT

      - name: Generate Test Report
        if: always()
        run: |
          # Define the report file path
          REPORT_FILE="test-results.json"
          
          # Initialize the JSON structure
          echo "{" > $REPORT_FILE
          echo "  \"package\": \"${{ steps.metadata.outputs.package_slug }}\"," >> $REPORT_FILE
          echo "  \"version\": \"${{ steps.version.outputs.version }}\"," >> $REPORT_FILE
          echo "  \"install_status\": \"${{ steps.install.outputs.install_status }}\"," >> $REPORT_FILE
          echo "  \"timestamp\": \"${{ steps.metadata.outputs.timestamp }}\"," >> $REPORT_FILE
          echo "  \"tests\": [" >> $REPORT_FILE
          
          # Collect test results
          TESTS=("test1" "test2" "test3" "test4" "test5")
          FIRST=true
          
          for TEST_ID in "${TESTS[@]}"; do
            STATUS="${{ steps[TEST_ID].outputs.status }}"
            DURATION="${{ steps[TEST_ID].outputs.duration }}"
            # Default to skipped if status is empty
            if [ -z "$STATUS" ]; then STATUS="skipped"; fi
            if [ -z "$DURATION" ]; then DURATION=0; fi
            
            if [ "$FIRST" = true ]; then
              FIRST=false
            else
              echo "," >> $REPORT_FILE
            fi
            
            echo "    {" >> $REPORT_FILE
            echo "      \"id\": \"$TEST_ID\"," >> $REPORT_FILE
            echo "      \"status\": \"$STATUS\"," >> $REPORT_FILE
            echo "      \"duration\": $DURATION" >> $REPORT_FILE
            echo "    }" >> $REPORT_FILE
          done
          
          echo "  ]" >> $REPORT_FILE
          echo "}" >> $REPORT_FILE
          
          # Pretty print the report
          cat $REPORT_FILE

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ steps.metadata.outputs.package_slug }}
          path: test-results.json

      - name: Update Test Results in Repository
        if: always()
        run: |
          # Create the results directory if it doesn't exist
          mkdir -p test_results/${{ steps.metadata.outputs.package_slug }}
          
          # Copy the report file to the results directory
          cp test-results.json test_results/${{ steps.metadata.outputs.package_slug }}/result.json
          
          # Configure git
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Add and commit the changes
          git add test_results/${{ steps.metadata.outputs.package_slug }}/result.json
          git commit -m "Update test results for ${{ steps.metadata.outputs.package_slug }}" || echo "No changes to commit"
          # git push is handled by the caller workflow or manually
