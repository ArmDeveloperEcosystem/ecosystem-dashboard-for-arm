---
draft: true
name: vLLM
category: AI/ML
description: vLLM is a fast, straightforward library for LLM inference and serving.
download_url: https://github.com/vllm-project/vllm/releases
works_on_arm: false
supported_minimum_version:
    version_number: 
    release_date: 


optional_info:
    homepage_url: https://github.com/vllm-project/vllm/
    support_caveats:
    alternative_options:
    getting_started_resources:
        arm_content:
        partner_content:
        official_docs: https://github.com/vllm-project/vllm/
    arm_recommended_minimum_version:
        version_number:
        release_date:


optional_hidden_info:
    release_notes__supported_minimum:
    release_notes__recommended_minimum:
    other_info: Does not currently support Arm. Requested feature in roadmap - https://github.com/vllm-project/vllm/issues/2681#issuecomment-1960696933 . Builds curenlty failing when trying, some have created their own dockerfiles- https://github.com/vllm-project/vllm/issues/2021

---
